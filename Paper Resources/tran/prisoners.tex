% LLNCS macro package for Springer Computer Science procedings;
% Version 2.20 of 2017/10/04
%\documentclass[runningheads]{llncs}
\documentclass[]{llncs} % Using this for now

\usepackage{graphicx} % Still need this for when you add images later
\usepackage{amsmath}  % For math (e.g., the n in N-person)
\usepackage{hyperref} % For clickable links
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=green,
    urlcolor=magenta
}

% For bibliography - llncs often has its own preferences.
% ieeetr is a good numbered style.
% If you want author-year and use natbib features like \citep, \citet, you'd uncomment:
%\usepackage[round,authoryear]{natbib} % Example for author-year with natbib
%\usepackage[numbers,sort&compress]{natbib} % Example for numbered with natbib

\begin{document}

\title{Navigating the N-Person Prisoner's Dilemma: From the Tragedy Valley to the Reciprocity Hill with Adaptive Learning Agents}
% \titlerunning{N-Person IPD: Tragedy Valley to Reciprocity Hill} % Uncomment if classictype is runningheads

\author{Chris Tcaci & Chris Huyck\inst{1}}
% \authorrunning{C. Tcaci} % Uncomment if classictype is runningheads
\institute{Middlesex University, London NW4 4BT UK\\
\email{M00674787@mdx.ac.uk} \and \email{c.huyck@mdx.ac.uk} \\
\url{https://cwa.mdx.ac.uk/chris/chrisroot.html}
}

\maketitle              

\begin{abstract}
The N-Person Iterated Prisoner's Dilemma (N-IPD) poses a significant challenge to the emergence of cooperation due to diffused responsibility and obscured reciprocity. This paper investigates how agent-based learning models navigate this complex social dilemma. We demonstrate that simple reinforcement learning agents consistently fall into a "Tragedy Valley" of mutual defection in standard N-IPD neighbourhood interaction models. However, by enhancing agents with contextual awareness of their local environment and employing adaptive Multi-Agent Reinforcement Learning (MARL) algorithms like Hysteretic-Q and Wolf-PHC, high levels of sustained cooperation (over 85\%) can be achieved. Furthermore, we explore the fundamental impact of interaction structure, contrasting the neighbourhood model with a pairwise interaction model where agents play repeated 2-player games. The pairwise model, by enabling direct reciprocity, facilitates a climb towards a "Reciprocity Hill," where cooperation is more readily established and maintained. Our findings highlight the critical roles of agent cognition, learning algorithms, and interaction structure in fostering cooperation in multi-agent systems.

\keywords{N-Person Prisoner's Dilemma \and Agent-Based Modelling \and Reinforcement Learning \and Emergence of Cooperation \and Tragedy Valley \and Reciprocity Hill \and Multi-Agent Systems.}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The Prisoner's Dilemma (PD) serves as a foundational paradigm in game theory, starkly illustrating the conflict between individual rational self-interest and mutually beneficial collective action \cite{Axelrod}. % Ensure 'Axelrod' key in prisoners.bib points to the 1984 book or a general IPD paper. Add a separate key for Axelrod & Hamilton 1981 if needed for that specific citation.
In its simplest form, two individuals, unable to communicate, must independently choose whether to cooperate or defect. While mutual cooperation yields a good outcome for both, each player has an individual incentive to defect, leading to a suboptimal outcome if both choose to do so. The Iterated Prisoner's Dilemma (IPD), where the game is played repeatedly, opens the door for cooperation to emerge through strategies based on reciprocity, as famously demonstrated by Axelrod's tournaments where Tit-for-Tat proved remarkably successful \cite{Axelrod}. % This could be Axelrod1980a or Axelrod1980b if you add those keys/entries.

However, many real-world social and economic dilemmas—ranging from managing common-pool resources to international climate agreements and team collaborations—involve more than two interacting parties. The N-Person Iterated Prisoner's Dilemma (N-IPD) generalizes the IPD to scenarios with $n$ participants \cite{Hamburger1973, Hardin1971}. % Add these to prisoners.bib
This extension introduces significant complexities:
\begin{itemize}
    \item \textbf{Diffused Responsibility and Payoffs:} The impact of a single agent's cooperative or defective action is spread across the group, diluting the direct consequences felt by any one individual.
    \item \textbf{Obscured Reciprocity:} It becomes harder to identify and respond to specific cooperators or defectors, making direct tit-for-tat like reciprocity challenging.
    \item \textbf{Increased Temptation to Free-Ride:} With many participants, an individual might be more tempted to defect, hoping to benefit from others' cooperation without contributing.
\end{itemize}

These complexities often lead rational, self-interested agents in N-IPD scenarios towards a "Tragedy Valley" of widespread defection, a concept echoing Hardin's "Tragedy of the Commons" \cite{Hardin1968}. % Add Hardin1968 to prisoners.bib
Our computational explorations using agent-based models (ABMs) with standard reinforcement learning (RL) agents consistently confirm this pessimistic outcome in certain N-IPD structures. This paper investigates the cognitive and structural conditions that allow learning agents to escape this valley and, in more favorable settings, ascend a "Reciprocity Hill" where cooperation can flourish.

We present \texttt{npdl}, an agent-based simulation framework, to explore these dynamics. Our central argument is that the emergence of cooperation in the N-IPD is not solely dependent on sophisticated learning algorithms but is critically shaped by (a) the agents' ability to perceive \textbf{context} from their social environment, (b) the inherent \textbf{adaptability} of their learning mechanisms, and (c) the fundamental **interaction structure** of the dilemma itself.

The key takeaways from our investigation are:
\begin{enumerate}
    \item \textbf{The "Tragedy Valley" vs. "Reciprocity Hill" (Interaction Structure - T1):} The structure of agent interactions is paramount.
        \begin{itemize}
            \item In N-IPD \textit{neighbourhood models}, where an agent's single choice affects a diffuse group payoff, most learning algorithms (including standard RL and simpler reactive strategies like Tit-for-Tat) tend to descend into the "Tragedy Valley" of defection.
            \item In contrast, N-IPD \textit{pairwise models}, where each agent effectively makes $N-1$ choices by engaging in distinct 2-player games with all others, direct reciprocity is clear. This structure facilitates climbing a "Reciprocity Hill" where cooperation is more readily established and maintained.
        \end{itemize}
    \item \textbf{Context is Crucial for Escaping the Valley (Cognitive Prerequisite - T2):} For agents operating in the challenging neighbourhood model, perceiving local social context (e.g., the proportion of cooperating neighbours) is a vital first step to avoid immediate and total defection.
    \item \textbf{Adaptive MARL Can Navigate the Valley (Learning Mechanism - T3):} Even within the difficult neighbourhood model, advanced Multi-Agent Reinforcement Learning (MARL) algorithms—particularly those incorporating optimism (like Hysteretic-Q) or adaptive learning rates (like Wolf-PHC)—can enable agents to learn resilient cooperative strategies and achieve high, sustained cooperation. Standard RL often fails where these succeed.
\end{enumerate}

This paper will first provide a brief background on the N-IPD and relevant learning approaches (Section \ref{sec:litreview}). We then describe the \texttt{npdl} simulation framework and its distinct interaction models (Section \ref{sec:framework}), followed by our experimental methodology (Section \ref{sec:methodology}). Results supporting our key takeaways are presented in Section \ref{sec:results}. Finally, we discuss the broader implications of these findings for understanding and fostering cooperation in multi-agent systems (Section \ref{sec:discussion}) and conclude with future research directions (Section \ref{sec:conclusion}).

\section{Background and Related Work}
\label{sec:litreview}

This section briefly reviews key concepts from game theory, the N-IPD, agent-based modelling, and reinforcement learning relevant to our study.
The work reported in this paper builds upon a broader understanding of learning and adaptation, though it diverges from prior work focused on spiking neural models such as those by \cite{Huyck, Huyck-Samey, HuyckErekpaine} and mechanisms derived from Diehl and Cook \cite{Diehl}, by focusing on abstract agent learning in game-theoretic scenarios.

\subsection{The N-Person Prisoner's Dilemma (N-IPD)}
The N-IPD extends the two-person dilemma to $N$ players. % Add Hamburger1973 reference (add to .bib)
Let $n_c$ be the number of players who choose to cooperate (C). The payoff to a cooperator is $P_C(n_c)$ and to a defector (D) is $P_D(n_c)$. The dilemma is characterized by:
\begin{itemize}
    \item \textbf{Dominance of Defection:} $P_D(n_c) > P_C(n_c+1)$ for all $0 \le n_c < N$. An individual always gains more by defecting.
    \item \textbf{Deficient Equilibrium:} $P_C(N) > P_D(0)$. Mutual cooperation is better for all than mutual defection.
\end{itemize}
This structure often leads to the "Tragedy of the Commons." % Add Hardin1968 reference (add to .bib) Our concept of the "Tragedy Valley" reflects this.

\subsection{Agent-Based Modelling (ABM) for Social Dilemmas}
Agent-Based Modelling (ABM) provides a bottom-up approach to studying complex systems by simulating autonomous agents. % Add Gilbert2007, Macal2010 references (add to .bib)
It is well-suited for exploring the N-IPD, allowing for heterogeneous strategies, local interactions, and emergent global patterns. Axelrod's tournaments for the 2-player IPD set a precedent \cite{Axelrod}. % Again, check key for Axelrod 1980/1981

\subsection{Reinforcement Learning in Multi-Agent Systems (MARL)}
Reinforcement Learning (RL) enables agents to learn optimal actions through trial-and-error. % Add SuttonBarto2018 reference (add to .bib)
Standard Q-learning faces challenges in multi-agent settings (MARL) due to non-stationarity. % Add Busoniu2008 reference (add to .bib)
To address these, we implemented:
\begin{itemize}
    \item \textbf{Hysteretic Q-learning:} Employs asymmetric, optimistic learning rates. % Add Matignon2007Hysteretic reference (add to .bib)
    \item \textbf{Win-or-Learn-Fast Policy Hill-Climbing (WoLF-PHC):} Adjusts learning rates based on performance. % Add Bowling2002WoLF reference (add to .bib)
\end{itemize}

\section{The \texttt{npdl} Simulation Framework and Interaction Models}
\label{sec:framework}
We developed \texttt{npdl}, a Python-based ABM platform. Key components include agent architecture and distinct interaction models.

\subsection{Agent Architecture}
Agents use learning strategies. Standard Q-learning agents perceive states based on their local neighbourhood. The \texttt{proportion_discretized} state representation, quantifying neighbour cooperation, is a critical contextual input. Advanced agents use Hysteretic-Q or Wolf-PHC.

\subsection{Interaction Models: Neighbourhood vs. Pairwise}
\texttt{npdl} simulates two N-IPD interaction structures:
\begin{enumerate}
    \item \textbf{Neighbourhood Model:} Agents interact with local network neighbours. Payoffs are from N-player functions based on neighbourhood cooperation. This represents diffuse public good scenarios and often leads to the "Tragedy Valley."
    %%% IMAGE GOES HERE: Conceptual Diagram of the Neighbourhood N-IPD Model. (Figure \ref{fig:neighbourhood_model_diagram})

    \item \textbf{Pairwise Model:} Each agent plays a 2-player IPD against every other agent. Total payoff sums these dyadic interactions. This emphasizes direct reciprocity, allowing strategies like Tit-for-Tat (TFT) to function effectively. This structure facilitates climbing the "Reciprocity Hill."
    %%% IMAGE GOES HERE: Conceptual Diagram of the Pairwise N-IPD Model. (Figure \ref{fig:pairwise_model_diagram})
\end{enumerate}
The pairwise model required careful agent memory handling for reactive strategies (per-opponent history) and RL agents (aggregate signals).

\section{Methodology and Experiments}
\label{sec:methodology}
Simulations typically involved $N=30$ agents, 500 rounds, Small-World networks, and standard PD payoffs ($R=3, S=0, T=5, P=1$).
We evaluated: \item Baseline Q-learning agents with minimal (\texttt{basic}) and contextual (\texttt{proportion_discretized}) state representations., optimized Hysteretic-Q and Wolf-PHC (often against TFT agents), global cooperation bonuses, and both Neighbourhood and Pairwise interaction models. Performance was measured by average cooperation rate.

\section{Results}
\label{sec:results}
This section presents key experimental results.

\subsection{The Tragedy Valley and the Importance of Context}
Standard Q-learning agents with a `basic` state (no neighbour information) rapidly converged to near-zero cooperation (the "Tragedy Valley").
%%% IMAGE GOES HERE: Baseline Performance: Q-learning with `basic` state. (Figure \ref{fig:baseline_basic_state})
Providing \texttt{proportion_discretized} state (fraction of cooperating neighbours) improved performance to unstable ~50\% cooperation. Context is critical but not solely sufficient.
%%% IMAGE GOES HERE: Impact of Minimal Context: Q-learning with `proportion_discretized` state. (Figure \ref{fig:baseline_prop_discr_state})

\subsection{Adaptive MARL Achieves High Cooperation in Neighbourhood N-IPD}
Optimized Hysteretic-Q and Wolf-PHC agents achieved high, sustained cooperation (over 85-90\%) in the N-IPD neighbourhood model, even against TFT agents. Hysteretic-Q's optimism and Wolf-PHC's adaptive learning rates were effective.
%%% IMAGE GOES HERE: Optimized Hysteretic-Q Performance. (Figure \ref{fig:hysq_performance})
%%% IMAGE GOES HERE: Optimized Wolf-PHC Performance. (Figure \ref{fig:wolfphc_performance})

\subsection{Impact of Interaction Structure: Pairwise Model and the Reciprocity Hill}
The pairwise interaction model, with explicit direct reciprocity, fundamentally alters the strategic landscape. Initial observations and theory suggest this structure makes the "Reciprocity Hill" more accessible, as feedback for cooperation/defection is immediate and unambiguous. RL agents benefit from clearer underlying reward signals.
%%% (Future work: comparative graphs for pairwise vs neighbourhood)

\section{Discussion}
\label{sec:discussion}
Our results highlight several key points. The **"Tragedy Valley"** is a common outcome in neighbourhood N-IPD for simple learners due to diffused incentives. **Contextual awareness** is crucial; agents need to perceive local social cues. **Adaptive MARL algorithms** like Hysteretic-Q and Wolf-PHC can overcome these challenges in neighbourhood N-IPD through sophisticated learning. Most fundamentally, **interaction structure** is a powerful determinant: the pairwise model's direct reciprocity creates a "Reciprocity Hill," making cooperation inherently easier to achieve and sustain than in the diffuse neighbourhood model.
The failure of standard exploration strategies like UCB1 (not shown here but detailed in original report) highlights non-stationarity challenges. Global incentives can also significantly boost cooperation.
Limitations include abstracted cognition and specific parameter choices.

\section{Conclusion}
\label{sec:conclusion}
This paper demonstrated that while N-IPD in neighbourhood models leads to a "Tragedy Valley" for simple learners, cooperation can emerge with **contextual awareness** and **adaptive MARL algorithms** (Hysteretic-Q, Wolf-PHC). The **interaction structure** is critical: pairwise models, facilitating a "Reciprocity Hill," make cooperation more accessible. The \texttt{npdl} framework enables these explorations. Future work will further investigate these dynamics to understand and promote cooperation in complex multi-agent systems.


% --- Bibliography ---
\bibliographystyle{ieeetr} % This is from your skeleton
\bibliography{prisoners}   % Your .bib file is named prisoners.bib
\end{document}