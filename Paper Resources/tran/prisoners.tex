% LLNCS macro package for Springer Computer Science procedings;
% Version 2.20 of 2017/10/04
%\documentclass[runningheads]{llncs}
\documentclass[]{llncs} % Using this for now

\usepackage{graphicx} % Still need this for when you add images later
\usepackage{amsmath}  % For math (e.g., the n in N-person)
\usepackage{hyperref} % For clickable links
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=green,
    urlcolor=magenta
}

% For bibliography - llncs often has its own preferences.
% ieeetr is a good numbered style.
% If you want author-year and use natbib features like \citep, \citet, you'd uncomment:
%\usepackage[round,authoryear]{natbib} % Example for author-year with natbib
%\usepackage[numbers,sort&compress]{natbib} % Example for numbered with natbib

\begin{document}

\title{Navigating the N-Person Prisoner's Dilemma: From the Tragedy Valley to the Reciprocity Hill with Adaptive Learning Agents}
% \titlerunning{N-Person IPD: Tragedy Valley to Reciprocity Hill} % Uncomment if classictype is runningheads

\author{Chris Tcaci & Chris Huyck\inst{1}}
% \authorrunning{C. Tcaci} % Uncomment if classictype is runningheads
\institute{Middlesex University, London NW4 4BT UK\\
\email{M00674787@mdx.ac.uk} \and \email{c.huyck@mdx.ac.uk} \\
\url{https://cwa.mdx.ac.uk/chris/chrisroot.html}
}

\maketitle              

\begin{abstract}
The N-Person Iterated Prisoner's Dilemma (N-IPD) poses a significant challenge to the emergence of cooperation due to diffused responsibility and obscured reciprocity. This paper investigates how agent-based learning models navigate this complex social dilemma. We demonstrate that simple reinforcement learning agents consistently fall into a "Tragedy Valley" of mutual defection in standard N-IPD neighbourhood interaction models. However, by enhancing agents with contextual awareness of their local environment and employing adaptive Multi-Agent Reinforcement Learning (MARL) algorithms like Hysteretic-Q and Wolf-PHC, high levels of sustained cooperation (over 85\%) can be achieved. Furthermore, we explore the fundamental impact of interaction structure, contrasting the neighbourhood model with a pairwise interaction model where agents play repeated 2-player games. The pairwise model, by enabling direct reciprocity, facilitates a climb towards a "Reciprocity Hill," where cooperation is more readily established and maintained. Our findings highlight the critical roles of agent cognition, learning algorithms, and interaction structure in fostering cooperation in multi-agent systems.

\keywords{N-Person Prisoner's Dilemma \and Agent-Based Modelling \and Reinforcement Learning \and Emergence of Cooperation \and Tragedy Valley \and Reciprocity Hill \and Multi-Agent Systems.}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The Prisoner's Dilemma (PD) serves as a foundational paradigm in game theory, starkly illustrating the conflict between individual rational self-interest and mutually beneficial collective action \cite{Axelrod}. % Ensure 'Axelrod' key in prisoners.bib points to the 1984 book or a general IPD paper. Add a separate key for Axelrod & Hamilton 1981 if needed for that specific citation.
In its simplest form, two individuals, unable to communicate, must independently choose whether to cooperate or defect. While mutual cooperation yields a good outcome for both, each player has an individual incentive to defect, leading to a suboptimal outcome if both choose to do so. The Iterated Prisoner's Dilemma (IPD), where the game is played repeatedly, opens the door for cooperation to emerge through strategies based on reciprocity, as famously demonstrated by Axelrod's tournaments where Tit-for-Tat proved remarkably successful \cite{Axelrod}. % This could be Axelrod1980a or Axelrod1980b if you add those keys/entries.

However, many real-world social and economic dilemmas—ranging from managing common-pool resources to international climate agreements and team collaborations—involve more than two interacting parties. The N-Person Iterated Prisoner's Dilemma (N-IPD) generalizes the IPD to scenarios with $n$ participants \cite{Hamburger1973, Hardin1971}. % Add these to prisoners.bib
This extension introduces significant complexities:
\begin{itemize}
    \item \textbf{Diffused Responsibility and Payoffs:} The impact of a single agent's cooperative or defective action is spread across the group, diluting the direct consequences felt by any one individual.
    \item \textbf{Obscured Reciprocity:} It becomes harder to identify and respond to specific cooperators or defectors, making direct tit-for-tat like reciprocity challenging.
    \item \textbf{Increased Temptation to Free-Ride:} With many participants, an individual might be more tempted to defect, hoping to benefit from others' cooperation without contributing.
\end{itemize}

These complexities often lead rational, self-interested agents in N-IPD scenarios towards a "Tragedy Valley" of widespread defection, a concept echoing Hardin's "Tragedy of the Commons" \cite{Hardin1968}. % Add Hardin1968 to prisoners.bib
Our computational explorations using agent-based models (ABMs) with standard reinforcement learning (RL) agents consistently confirm this pessimistic outcome in certain N-IPD structures. This paper investigates the cognitive and structural conditions that allow learning agents to escape this valley and, in more favorable settings, ascend a "Reciprocity Hill" where cooperation can flourish.

We present \texttt{npdl}, an agent-based simulation framework, to explore these dynamics. Our central argument is that the emergence of cooperation in the N-IPD is not solely dependent on sophisticated learning algorithms but is critically shaped by (a) the agents' ability to perceive \textbf{context} from their social environment, (b) the inherent \textbf{adaptability} of their learning mechanisms, and (c) the fundamental **interaction structure** of the dilemma itself.

The key takeaways from our investigation are:
\begin{enumerate}
    \item \textbf{The "Tragedy Valley" vs. "Reciprocity Hill" (Interaction Structure - T1):} The structure of agent interactions is paramount.
        \begin{itemize}
            \item In N-IPD \textit{neighbourhood models}, where an agent's single choice affects a diffuse group payoff, most learning algorithms (including standard RL and simpler reactive strategies like Tit-for-Tat) tend to descend into the "Tragedy Valley" of defection.
            \item In contrast, N-IPD \textit{pairwise models}, where each agent effectively makes $N-1$ choices by engaging in distinct 2-player games with all others, direct reciprocity is clear. This structure facilitates climbing a "Reciprocity Hill" where cooperation is more readily established and maintained.
        \end{itemize}
    \item \textbf{Context is Crucial for Escaping the Valley (Cognitive Prerequisite - T2):} For agents operating in the challenging neighbourhood model, perceiving local social context (e.g., the proportion of cooperating neighbours) is a vital first step to avoid immediate and total defection.
    \item \textbf{Adaptive MARL Can Navigate the Valley (Learning Mechanism - T3):} Even within the difficult neighbourhood model, advanced Multi-Agent Reinforcement Learning (MARL) algorithms—particularly those incorporating optimism (like Hysteretic-Q) or adaptive learning rates (like Wolf-PHC)—can enable agents to learn resilient cooperative strategies and achieve high, sustained cooperation. Standard RL often fails where these succeed.
\end{enumerate}

This paper will first provide a brief background on the N-IPD and relevant learning approaches (Section \ref{sec:litreview}). We then describe the \texttt{npdl} simulation framework and its distinct interaction models (Section \ref{sec:framework}), followed by our experimental methodology (Section \ref{sec:methodology}). Results supporting our key takeaways are presented in Section \ref{sec:results}. Finally, we discuss the broader implications of these findings for understanding and fostering cooperation in multi-agent systems (Section \ref{sec:discussion}) and conclude with future research directions (Section \ref{sec:conclusion}).

\section{Background and Related Work}
\label{sec:litreview}

This section briefly reviews key concepts from game theory, focusing on the N-Person Prisoner's Dilemma (N-IPD), and introduces the Agent-Based Modelling (ABM) and Multi-Agent Reinforcement Learning (MARL) approaches employed in our study. 

\subsection{The N-Person Prisoner's Dilemma (N-IPD)}
The N-IPD extends the classic two-person dilemma to $N \ge 2$ players, presenting a more complex challenge for cooperation \cite{Hamburger1973, Hardin1971}. % Ensure Hamburger1973 and Hardin1971 are in prisoners.bib with these keys.
Let $n_c$ be the number of players in a group who choose to cooperate (C), and $N-n_c$ be the number who choose to defect (D). The payoff to an individual cooperator is $P_C(n_c)$ and to an individual defector is $P_D(n_c-1)$ (when considering a defector, they are not part of the $n_c$ cooperators in their own payoff calculation from that group). The dilemma is typically characterized by two conditions:
\begin{itemize}
    \item \textbf{Dominance of Defection:} For any individual, defecting yields a higher personal payoff than cooperating, regardless of how many others cooperate. Formally, if an agent considers switching from cooperate to defect, its payoff increases: $P_D(n_c) > P_C(n_c+1)$ where $n_c$ is the number of *other* cooperators (if the agent defects) versus $n_c+1$ (if the agent cooperates).
    \item \textbf{Deficient Equilibrium:} Universal cooperation yields a better payoff for every individual than universal defection: $P_C(N) > P_D(0)$.
\end{itemize}
This inherent conflict between individual rationality (defect) and collective benefit (cooperate) often leads to the "Tragedy of the Commons" \cite{Hardin1968}, where shared resources are depleted. Our concept of the "Tragedy Valley" directly reflects this gravitational pull towards mutual defection observed in N-IPD simulations.

\subsection{Agent-Based Modelling (ABM) for Social Dilemmas}
Agent-Based Modelling (ABM) offers a powerful computational methodology for studying complex social systems from the bottom up \cite{Gilbert2007, Macal2010}. % Add Gilbert2007 and Macal2010 to prisoners.bib
By simulating the actions and interactions of autonomous, heterogeneous agents according to predefined rules within a specified environment, ABM allows researchers to observe emergent macroscopic phenomena, such as the rise or fall of cooperation. It is particularly well-suited for exploring the N-IPD due to its ability to model local interactions, diverse agent strategies (including learning), and the non-linear dynamics that often characterize social dilemmas. Axelrod's pioneering tournaments using ABM for the 2-player IPD provided early insights into the conditions favoring cooperative strategies \cite{Axelrod}.

\subsection{Reinforcement Learning in Multi-Agent Systems (MARL)}
Reinforcement Learning (RL) is a class of machine learning where agents learn to make optimal sequences of decisions by interacting with an environment and receiving feedback in the form of rewards or punishments \cite{SuttonBarto2018}. % Add SuttonBarto2018 to prisoners.bib
Standard Q-learning is a foundational RL algorithm that learns the value of taking a particular action in a given state. However, when applied to multi-agent systems (MARL), where multiple agents are learning simultaneously, standard RL algorithms face significant challenges, primarily due to the non-stationarity of the environment: each agent's policy changes as it learns, thereby changing the environment from the perspective of other agents \cite{Busoniu2008}. % Add Busoniu2008 to prisoners.bib
This can destabilize learning and prevent convergence to cooperative equilibria. To address these issues within the N-IPD context, our work explores more advanced MARL techniques:
\begin{itemize}
    \item \textbf{Hysteretic Q-learning:} This algorithm employs asymmetric learning rates, specifically using a higher learning rate for positive updates (when an action leads to a better-than-expected outcome) and a lower one for negative updates. This "optimism" can help sustain cooperation by making agents less reactive to occasional defections and quicker to reinforce mutually beneficial actions \cite{Matignon2007Hysteretic}. % Add Matignon2007Hysteretic to prisoners.bib
    \item \textbf{Win-or-Learn-Fast Policy Hill-Climbing (WoLF-PHC):} WoLF-PHC dynamically adjusts an agent's learning rate based on its performance relative to an average policy. If an agent is "winning" (performing better than average), it learns more cautiously (lower learning rate); if "losing," it learns more rapidly (higher learning rate). This adaptability helps agents converge in non-stationary settings \cite{Bowling2002WoLF}. % Add Bowling2002WoLF to prisoners.bib
\end{itemize}
These algorithms represent attempts to endow agents with more sophisticated learning capabilities to navigate the complexities of multi-agent interactions.

\section{The \texttt{npdl} Simulation Framework and Interaction Models}
\label{sec:framework}
We developed \texttt{npdl}, a Python-based ABM platform. Key components include agent architecture and distinct interaction models.

\subsection{Agent Architecture}
Agents use learning strategies. Standard Q-learning agents perceive states based on their local neighbourhood. The \texttt{proportion_discretized} state representation, quantifying neighbour cooperation, is a critical contextual input. Advanced agents use Hysteretic-Q or Wolf-PHC.

\subsection{Interaction Models: Neighbourhood vs. Pairwise}
\texttt{npdl} simulates two N-IPD interaction structures:
\begin{enumerate}
    \item \textbf{Neighbourhood Model:} Agents interact with local network neighbours. Payoffs are from N-player functions based on neighbourhood cooperation. This represents diffuse public good scenarios and often leads to the "Tragedy Valley."
    %%% IMAGE GOES HERE: Conceptual Diagram of the Neighbourhood N-IPD Model. (Figure \ref{fig:neighbourhood_model_diagram})

    \item \textbf{Pairwise Model:} Each agent plays a 2-player IPD against every other agent. Total payoff sums these dyadic interactions. This emphasizes direct reciprocity, allowing strategies like Tit-for-Tat (TFT) to function effectively. This structure facilitates climbing the "Reciprocity Hill."
    %%% IMAGE GOES HERE: Conceptual Diagram of the Pairwise N-IPD Model. (Figure \ref{fig:pairwise_model_diagram})
\end{enumerate}
The pairwise model required careful agent memory handling for reactive strategies (per-opponent history) and RL agents (aggregate signals).

\section{Methodology and Experiments}
\label{sec:methodology}
Simulations typically involved $N=30$ agents, 500 rounds, Small-World networks, and standard PD payoffs ($R=3, S=0, T=5, P=1$).
We evaluated: \item Baseline Q-learning agents with minimal (\texttt{basic}) and contextual (\texttt{proportion_discretized}) state representations., optimized Hysteretic-Q and Wolf-PHC (often against TFT agents), global cooperation bonuses, and both Neighbourhood and Pairwise interaction models. Performance was measured by average cooperation rate.

\section{Results}
\label{sec:results}
This section presents key experimental results.

\subsection{The Tragedy Valley and the Importance of Context}
Standard Q-learning agents with a `basic` state (no neighbour information) rapidly converged to near-zero cooperation (the "Tragedy Valley").
%%% IMAGE GOES HERE: Baseline Performance: Q-learning with `basic` state. (Figure \ref{fig:baseline_basic_state})
Providing \texttt{proportion_discretized} state (fraction of cooperating neighbours) improved performance to unstable ~50\% cooperation. Context is critical but not solely sufficient.
%%% IMAGE GOES HERE: Impact of Minimal Context: Q-learning with `proportion_discretized` state. (Figure \ref{fig:baseline_prop_discr_state})

\subsection{Adaptive MARL Achieves High Cooperation in Neighbourhood N-IPD}
Optimized Hysteretic-Q and Wolf-PHC agents achieved high, sustained cooperation (over 85-90\%) in the N-IPD neighbourhood model, even against TFT agents. Hysteretic-Q's optimism and Wolf-PHC's adaptive learning rates were effective.
%%% IMAGE GOES HERE: Optimized Hysteretic-Q Performance. (Figure \ref{fig:hysq_performance})
%%% IMAGE GOES HERE: Optimized Wolf-PHC Performance. (Figure \ref{fig:wolfphc_performance})

\subsection{Impact of Interaction Structure: Pairwise Model and the Reciprocity Hill}
The pairwise interaction model, with explicit direct reciprocity, fundamentally alters the strategic landscape. Initial observations and theory suggest this structure makes the "Reciprocity Hill" more accessible, as feedback for cooperation/defection is immediate and unambiguous. RL agents benefit from clearer underlying reward signals.
%%% (Future work: comparative graphs for pairwise vs neighbourhood)

\section{Discussion}
\label{sec:discussion}
Our experimental results shed light on the critical factors influencing the emergence and stability of cooperation in the N-Person Iterated Prisoner's Dilemma, painting a narrative of challenges and pathways towards collective benefit. The concepts of the "Tragedy Valley" and the "Reciprocity Hill" serve as useful metaphors for the different dynamic landscapes agents encounter.

\textbf{T1: Interaction Structure as the Primary Determinant – The Valley and The Hill.}
Perhaps our most fundamental insight is the profound impact of the interaction structure itself.
The standard N-IPD \textit{neighbourhood model}, where an agent makes a single choice and its payoff is determined by the collective actions within its local group, inherently presents a difficult path to cooperation. The benefits of an individual's cooperation are diffused, while the costs are borne individually. Direct, targeted reciprocity is obscured, making it hard for simple reciprocal strategies like Tit-for-Tat to gain a foothold or for learners to accurately assign credit for good outcomes. This environment readily leads agents into the "Tragedy Valley" of mutual defection. Even if agents possess some learning capabilities, the path out of this valley is steep and fraught with the risk of being exploited.

In stark contrast, the \textit{pairwise model} of N-IPD fundamentally alters this landscape. Here, each agent effectively engages in $N-1$ distinct 2-player IPD games with every other participant. This structure brings clarity and directness to reciprocity. A defection from agent B towards agent A directly impacts A's payoff from that specific interaction, and A can retaliate or forgive B in their subsequent dyadic game without "punishing" other innocent bystanders. This clear cause-and-effect makes it much easier for cooperative norms, supported by reciprocal strategies, to emerge and stabilize. The "Reciprocity Hill" becomes a more accessible and sustainable state because individual incentives are better aligned with mutual cooperation through direct, accountable interactions. Our framework's ability to model both structures allowed us to highlight this critical difference.

\textbf{T2: Contextual Awareness – A Perceptual Foothold Against the Valley's Slope.}
For agents operating within the challenging neighbourhood model, the ability to perceive their immediate social context is a crucial first defense against an immediate slide into the Tragedy Valley. Our results (Section \ref{sec:results}A) clearly showed that Q-learning agents with no information about their neighbours' actions (`basic` state) invariably defected. However, simply providing them with a discretized proportion of cooperating neighbours (`proportion_discretized` state) allowed for a significantly higher, albeit unstable, level of cooperation. This suggests that even rudimentary social awareness—knowing generally "what others are doing"—is a prerequisite for breaking cycles of pure defection. It provides a perceptual foothold, allowing agents to at least react to, rather than ignore, the collective behaviour of their peers, though it is not sufficient on its own to ensure high cooperation.

\textbf{T3: Adaptive MARL – The Cognitive Tools to Climb in Difficult Terrain.}
While context is necessary, more sophisticated cognitive tools in the form of adaptive learning are often required to navigate the N-IPD neighbourhood model successfully and sustain high levels of cooperation. Standard reinforcement learning, represented by basic Q-learning, struggles with the non-stationarity of the multi-agent environment. In contrast, adaptive MARL algorithms like Hysteretic-Q and Wolf-PHC demonstrated the capacity to achieve and maintain robust cooperation (Section \ref{sec:results}B).
Hysteretic-Q, with its optimistic approach of learning more readily from positive experiences (mutual cooperation) than from negative ones (being defected upon), fosters resilience. It allows agents to "forgive" occasional defections and maintain cooperative overtures, preventing spirals of retaliation that can trap agents in the Tragedy Valley.
Wolf-PHC, by dynamically adjusting its learning rates based on whether it is "winning" or "losing," shows adaptability to the changing strategies of other agents. This careful modulation of learning helps it find and stabilize cooperative equilibria that elude simpler learners.
These algorithms, therefore, represent mechanisms by which agents can, through more nuanced learning, not only avoid the worst of the Tragedy Valley but actively "climb" towards more cooperative states even in the challenging diffuse-payoff structure of the neighbourhood N-IPD. Their success underscores that the type of learning matters significantly; it's not just about learning, but *how* agents learn in a social context.

The consistent failure of standard exploration strategies like UCB1 (detailed in the original report but not explicitly shown with a figure here) further emphasizes the difficulties posed by non-stationarity in MARL within the N-IPD. Additionally, while external factors like global incentives can dramatically shift the landscape towards cooperation (as also shown in our broader study), the focus of these core takeaways is on the inherent learnability and structural properties of the dilemma itself and the cognitive capabilities of the agents.
Our findings collectively suggest that understanding and promoting cooperation in complex multi-agent systems requires attention not only to individual learning capacities but also, critically, to the very structure of their interactions. Limitations of this study include the abstraction of agent cognition and the focus on specific network structures and payoff parameters. Future work will expand the range of scenarios and delve deeper into the comparative dynamics of the neighbourhood versus pairwise models, further exploring the pathways from the Tragedy Valley to the Reciprocity Hill.

\section{Conclusion}
\label{sec:conclusion}
This paper demonstrated that while N-IPD in neighbourhood models leads to a "Tragedy Valley" for simple learners, cooperation can emerge with **contextual awareness** and **adaptive MARL algorithms** (Hysteretic-Q, Wolf-PHC). The **interaction structure** is critical: pairwise models, facilitating a "Reciprocity Hill," make cooperation more accessible. The \texttt{npdl} framework enables these explorations. Future work will further investigate these dynamics to understand and promote cooperation in complex multi-agent systems.


% --- Bibliography ---
\bibliographystyle{ieeetr} % This is from your skeleton
\bibliography{prisoners}   % Your .bib file is named prisoners.bib
\end{document}