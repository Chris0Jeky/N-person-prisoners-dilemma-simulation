================================================================================
                    SIMULATION CONFIGURATION REPORT
================================================================================

Generated: 2025-06-24 10:13:12
Output Directory: final_comparison_charts

SIMULATION PARAMETERS
----------------------------------------
Number of rounds per simulation: 100
Number of runs to average: 200
Total simulations per scenario: 20,000

VANILLA Q-LEARNER (Fixed Parameters)
----------------------------------------
Learning Rate (α): 0.1
Discount Factor (γ): 0.95
Exploration Rate (ε): 0.1

ADAPTIVE Q-LEARNER (Dynamic Parameters)
----------------------------------------
Initial Learning Rate: 0.1
Initial Exploration Rate: 0.15
Learning Rate Range: [0.03, 0.15]
Exploration Rate Range: [0.02, 0.15]
Adaptation Factor: 1.08
Reward Window Size: 75
Discount Factor (γ): 0.95

HYSTERETIC Q-LEARNER (Asymmetric Learning)
----------------------------------------
Learning Rate (α) for good news: 0.12
Learning Rate (β) for bad news: 0.002
Discount Factor (γ): 0.95
Exploration Rate (ε): 0.05

LEGACY Q-LEARNER (Sophisticated State Representation)
----------------------------------------
Learning Rate (α): 0.15
Discount Factor (γ): 0.95
Initial Exploration Rate (ε): 0.3
Epsilon Decay Rate: 0.995
Minimum Epsilon: 0.05
Initial Q-values: 0.0
Features: 2-round history, cooperation trends

LEGACY 3-ROUND Q-LEARNER (Extended History)
----------------------------------------
Learning Rate (α): 0.15
Discount Factor (γ): 0.99
Initial Exploration Rate (ε): 0.25
Epsilon Decay Rate: 0.998
Minimum Epsilon: 0.01
Initial Q-values: -0.3
History Length: 3 rounds
Features: 3-round history, complex trend analysis, pessimistic initialization

SCENARIO DESCRIPTIONS
----------------------------------------
2QL_vs_1AllC: Two Q-learners compete against one Always Cooperate agent
2QL_vs_1AllD: Two Q-learners compete against one Always Defect agent
2QL_vs_1Random: Two Q-learners compete against one Random agent
2QL_vs_1TFT: Two Q-learners compete against one Tit-for-Tat agent
2QL_vs_1TFT-E: Two Q-learners compete against one TFT with 10% error rate
1QL_vs_2AllC: One Q-learner competes against two Always Cooperate agents
1QL_vs_2AllD: One Q-learner competes against two Always Defect agents
1QL_vs_2Random: One Q-learner competes against two Random agents
1QL_vs_2TFT: One Q-learner competes against two Tit-for-Tat agents
1QL_vs_2TFT-E: One Q-learner competes against two TFT with 10% error rate

GAME PARAMETERS
----------------------------------------
Prisoner's Dilemma Payoffs:
  T (Temptation): 5
  R (Reward): 3
  P (Punishment): 1
  S (Sucker): 0

Agent Strategies:
  AllC: Always Cooperate
  AllD: Always Defect
  Random: 50/50 random choice
  TFT: Tit-for-Tat
    - Pairwise: Copies opponent's last move
    - N-Person: Cooperates with probability = cooperation ratio
  TFT-E: TFT with 10% error rate

================================================================================
