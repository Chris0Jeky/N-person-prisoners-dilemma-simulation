[
  {
    "scenario_name": "Evo_1744501756_8140",
    "num_agents": 50,
    "num_rounds": 500,
    "network_type": "small_world",
    "network_params": {
      "k": 6,
      "beta": 0.2
    },
    "interaction_mode": "pairwise",
    "agent_strategies": {
      "lra_q": 35,
      "tit_for_tat": 6,
      "pavlov": 9,
      "q_learning_adaptive": 27
    },
    "payoff_type": "linear",
    "payoff_params": {
      "S": 0,
      "T": 6,
      "P": 1,
      "R": 4
    },
    "state_type": "memory_enhanced",
    "memory_length": 10,
    "learning_rate": 0.05,
    "discount_factor": 0.9,
    "epsilon": 0.1,
    "q_init_type": "zero",
    "logging_interval": 501,
    "increase_rate": 0.171,
    "decrease_rate": 0.023,
    "exploration_constant": 1.5,
    "rewiring_prob": 0.08
  },
  {
    "scenario_name": "Evo_1744501752_5144",
    "num_agents": 40,
    "num_rounds": 500,
    "network_type": "small_world",
    "network_params": {
      "k": 6,
      "beta": 0.2
    },
    "interaction_mode": "pairwise",
    "agent_strategies": {
      "ucb1_q": 5,
      "tit_for_tat": 54,
      "hysteretic_q": 8
    },
    "payoff_type": "linear",
    "payoff_params": {
      "R": 4,
      "S": 0,
      "T": 5,
      "P": 1
    },
    "state_type": "memory_enhanced",
    "memory_length": 3,
    "learning_rate": 0.05,
    "discount_factor": 0.99,
    "epsilon": 0.1,
    "q_init_type": "random",
    "logging_interval": 501,
    "exploration_constant": 1.5,
    "use_global_bonus": true
  },
  {
    "scenario_name": "Evo_1744501749_5194",
    "num_agents": 50,
    "num_rounds": 500,
    "network_type": "small_world",
    "network_params": {
      "k": 6,
      "beta": 0.2
    },
    "interaction_mode": "pairwise",
    "agent_strategies": {
      "lra_q": 35,
      "tit_for_tat": 6,
      "pavlov": 9,
      "q_learning_adaptive": 27
    },
    "payoff_type": "linear",
    "payoff_params": {
      "S": 0,
      "T": 6,
      "P": 1,
      "R": 3
    },
    "state_type": "memory_enhanced",
    "memory_length": 10,
    "learning_rate": 0.05,
    "discount_factor": 0.99,
    "epsilon": 0.1,
    "q_init_type": "zero",
    "logging_interval": 201,
    "increase_rate": 0.171,
    "decrease_rate": 0.023,
    "rewiring_interval": 20,
    "rewiring_prob": 0.08
  },
  {
    "scenario_name": "Evo_1744501745_5664",
    "num_agents": 40,
    "num_rounds": 500,
    "network_type": "random",
    "network_params": {
      "probability": 0.7
    },
    "interaction_mode": "pairwise",
    "agent_strategies": {
      "ucb1_q": 13,
      "tit_for_tat": 54
    },
    "payoff_type": "linear",
    "payoff_params": {
      "S": 0,
      "T": 6,
      "P": 1,
      "R": 3
    },
    "state_type": "memory_enhanced",
    "memory_length": 3,
    "learning_rate": 0.05,
    "discount_factor": 0.99,
    "epsilon": 0.1,
    "q_init_type": "random",
    "logging_interval": 501,
    "exploration_constant": 1.5,
    "use_global_bonus": true
  },
  {
    "scenario_name": "GeneratedScenario_0011",
    "num_agents": 50,
    "num_rounds": 500,
    "network_type": "small_world",
    "network_params": {
      "k": 6,
      "beta": 0.2
    },
    "interaction_mode": "pairwise",
    "agent_strategies": {
      "tit_for_tat": 6,
      "pavlov": 9,
      "lra_q": 8,
      "q_learning_adaptive": 27
    },
    "payoff_type": "linear",
    "payoff_params": {
      "R": 3,
      "S": 0,
      "T": 6,
      "P": 1
    },
    "state_type": "memory_enhanced",
    "memory_length": 20,
    "q_init_type": "zero",
    "learning_rate": 0.05,
    "discount_factor": 0.99,
    "epsilon": 0.1,
    "increase_rate": 0.171,
    "decrease_rate": 0.023,
    "rewiring_interval": 20,
    "rewiring_prob": 0.08,
    "logging_interval": 501
  }
]