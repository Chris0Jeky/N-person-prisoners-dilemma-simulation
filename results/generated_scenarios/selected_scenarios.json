[
  {
    "scenario_name": "GeneratedScenario_0003",
    "num_agents": 20,
    "num_rounds": 500,
    "network_type": "scale_free",
    "network_params": {
      "m": 3
    },
    "interaction_mode": "neighborhood",
    "agent_strategies": {
      "wolf_phc": 4,
      "q_learning": 4,
      "q_learning_adaptive": 5,
      "hysteretic_q": 7
    },
    "payoff_type": "linear",
    "payoff_params": {
      "R": 3,
      "S": -1,
      "T": 5,
      "P": 0
    },
    "state_type": "proportion_discretized",
    "memory_length": 20,
    "q_init_type": "zero",
    "learning_rate": 0.05,
    "discount_factor": 0.99,
    "epsilon": 0.3,
    "beta": 0.0328,
    "alpha_win": 0.083,
    "alpha_lose": 0.291,
    "logging_interval": 501
  },
  {
    "scenario_name": "GeneratedScenario_0010",
    "num_agents": 50,
    "num_rounds": 500,
    "network_type": "scale_free",
    "network_params": {
      "m": 2
    },
    "interaction_mode": "pairwise",
    "agent_strategies": {
      "q_learning_adaptive": 1,
      "tit_for_two_tats": 4,
      "lra_q": 45
    },
    "payoff_type": "linear",
    "payoff_params": {
      "R": 3,
      "S": 0,
      "T": 5,
      "P": 1
    },
    "state_type": "proportion_discretized",
    "memory_length": 10,
    "q_init_type": "random",
    "learning_rate": 0.05,
    "discount_factor": 0.9,
    "epsilon": 0.05,
    "increase_rate": 0.115,
    "decrease_rate": 0.042,
    "logging_interval": 501
  },
  {
    "scenario_name": "GeneratedScenario_0015",
    "num_agents": 40,
    "num_rounds": 200,
    "network_type": "scale_free",
    "network_params": {
      "m": 2
    },
    "interaction_mode": "neighborhood",
    "agent_strategies": {
      "always_defect": 7,
      "wolf_phc": 7,
      "lra_q": 26
    },
    "payoff_type": "linear",
    "payoff_params": {
      "R": 4,
      "S": 0,
      "T": 5,
      "P": 1
    },
    "state_type": "memory_enhanced",
    "memory_length": 10,
    "q_init_type": "random",
    "learning_rate": 0.2,
    "discount_factor": 0.99,
    "epsilon": 0.2,
    "increase_rate": 0.16,
    "decrease_rate": 0.042,
    "alpha_win": 0.012,
    "alpha_lose": 0.27,
    "logging_interval": 201
  },
  {
    "scenario_name": "GeneratedScenario_0023",
    "num_agents": 20,
    "num_rounds": 200,
    "network_type": "scale_free",
    "network_params": {
      "m": 2
    },
    "interaction_mode": "pairwise",
    "agent_strategies": {
      "generous_tit_for_tat": 3,
      "q_learning": 4,
      "lra_q": 1,
      "ucb1_q": 12
    },
    "payoff_type": "linear",
    "payoff_params": {
      "R": 3,
      "S": 0,
      "T": 6,
      "P": 1
    },
    "state_type": "count",
    "memory_length": 10,
    "q_init_type": "random",
    "learning_rate": 0.2,
    "discount_factor": 0.8,
    "epsilon": 0.2,
    "increase_rate": 0.143,
    "decrease_rate": 0.081,
    "exploration_constant": 2.6,
    "generosity": 0.19,
    "logging_interval": 201
  },
  {
    "scenario_name": "GeneratedScenario_0000",
    "num_agents": 20,
    "num_rounds": 500,
    "network_type": "scale_free",
    "network_params": {
      "m": 3
    },
    "interaction_mode": "neighborhood",
    "agent_strategies": {
      "wolf_phc": 8,
      "generous_tit_for_tat": 12
    },
    "payoff_type": "linear",
    "payoff_params": {
      "R": 3,
      "S": 0,
      "T": 5,
      "P": 1
    },
    "state_type": "memory_enhanced",
    "memory_length": 10,
    "q_init_type": "zero",
    "learning_rate": 0.2,
    "discount_factor": 0.8,
    "epsilon": 0.3,
    "alpha_win": 0.059,
    "alpha_lose": 0.261,
    "generosity": 0.16,
    "logging_interval": 501
  }
]